<!DOCTYPE html><!--ytgr4ScHVKxxVOmg1QY7t--><html lang="en" class="geistmono_157ca88a-module__TXdLRq__variable"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/GeistMono_Variable.p.73882635.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/images/deep-q-learning/animation.gif" as="image"/><link rel="preload" href="/images/deep-q-learning/rl.png" as="image"/><link rel="preload" href="/images/deep-q-learning/atari.png" as="image"/><link rel="preload" href="/images/deep-q-learning/neuralnet.png" as="image"/><link rel="preload" href="/images/deep-q-learning/deep-q-learning.png" as="image"/><link rel="preload" href="/images/deep-q-learning/100.png" as="image"/><link rel="preload" href="/images/deep-q-learning/300.png" as="image"/><link rel="stylesheet" href="/_next/static/chunks/abf9089aed6bac0a.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/62c7aeefb8086c23.js"/><script src="/_next/static/chunks/142337fd25442fe6.js" async=""></script><script src="/_next/static/chunks/406ec2b483ec95d1.js" async=""></script><script src="/_next/static/chunks/bea56052aeffac28.js" async=""></script><script src="/_next/static/chunks/turbopack-5ed8a75e49c501f4.js" async=""></script><script src="/_next/static/chunks/7f64245c6154235a.js" async=""></script><script src="/_next/static/chunks/1ca67093247280ae.js" async=""></script><script src="/_next/static/chunks/fec5ea6183d0e55b.js" async=""></script><script src="/_next/static/chunks/3b60e5197848a126.js" async=""></script><title>Keon Kim</title><meta name="description" content="Keon&#x27;s blog."/><link rel="manifest" href="/favicon/site.webmanifest"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="Keon Kim"/><meta property="og:description" content="Keon&#x27;s blog."/><meta property="og:url" content="https://keon.github.io/"/><meta property="og:site_name" content="Keon Kim"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Keon Kim"/><meta name="twitter:description" content="Keon&#x27;s blog."/><link rel="icon" href="/favicon/favicon-16x16.png" sizes="16x16" type="image/png"/><link rel="icon" href="/favicon/favicon-32x32.png" sizes="32x32" type="image/png"/><link rel="icon" href="/favicon/favicon.ico" sizes="any"/><link rel="apple-touch-icon" href="/favicon/apple-touch-icon.png" sizes="180x180" type="image/png"/><link rel="android-chrome-192x192" href="/favicon/android-chrome-192x192.png"/><link rel="android-chrome-512x512" href="/favicon/android-chrome-512x512.png"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="font-mono antialiased max-w-2xl mx-4 mt-6 lg:mx-auto text-sm text-black bg-white dark:text-white dark:bg-black"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><main class="flex-auto min-w-0 mt-4 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-8 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-center justify-between relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:bg-black hover:text-white dark:hover:bg-white dark:hover:text-black flex align-middle relative py-1 px-2 m-1" href="/">home</a><a class="transition-all hover:bg-black hover:text-white dark:hover:bg-white dark:hover:text-black flex align-middle relative py-1 px-2 m-1" href="/writing/">writing</a></div></nav></div></aside><section><h1 class="title font-bold text-lg">Deep Q-Learning with Keras and Gym</h1><div class="flex justify-between items-center mt-1 mb-6 text-xs"><span class="relative group cursor-default inline-flex items-center"><span class="absolute bottom-full left-0 mb-1 px-2 py-1 text-[10px] bg-black text-white dark:bg-white dark:text-black invisible group-hover:visible transition-all pointer-events-none whitespace-nowrap">Feb 6, 2017, 24:00<br/>Feb 6, 2017, 24:00<!-- --> GMT</span></span></div><article class="prose"><div style="display:flex;justify-content:center;margin-bottom:2rem"><img src="/images/deep-q-learning/animation.gif" alt="CartPole Animation"/></div>
<p>This blog post will demonstrate how deep reinforcement learning (deep Q-learning) can be implemented and applied to play a CartPole game using Keras and Gym, <strong>in less than 100 lines of code</strong>!</p>
<p>I&#x27;ll explain everything without requiring any prerequisite knowledge about reinforcement learning.</p>
<p>The code used for this article is on <a target="_blank" rel="noopener noreferrer" href="https://github.com/keon/deep-q-learning">GitHub</a>.</p>
<h2 id="reinforcement-learning"><a href="#reinforcement-learning" class="anchor"></a>Reinforcement Learning</h2>
<p><img src="/images/deep-q-learning/rl.png" alt="Reinforcement Learning"/></p>
<p><em>Reinforcement Learning</em> is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Just like how we learn to ride a bicycle, this kind of AI learns by trial and error. As seen in the picture, the brain represents the AI agent, which acts on the environment. After each action, the agent receives the feedback. The feedback consists of the reward and next state of the environment. The reward is usually defined by a human. If we use the analogy of the bicycle, we can define reward as the distance from the original starting point.</p>
<h2 id="deep-reinforcement-learning"><a href="#deep-reinforcement-learning" class="anchor"></a>Deep Reinforcement Learning</h2>
<p>Google&#x27;s DeepMind published its famous paper <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1312.5602"><em>Playing Atari with Deep Reinforcement Learning</em></a>, in which they introduced a new algorithm called <strong>Deep Q Network</strong> (DQN for short) in 2013. It demonstrated how an AI agent can learn to play games by just observing the screen without any prior information about those games. The result turned out to be pretty impressive. This paper opened the era of what is called &#x27;deep reinforcement learning&#x27;, a mix of deep learning and reinforcement learning.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=V1eYniJ0Rnk">Click to Watch: DeepMind&#x27;s Atari Player</a></p>
<p><img src="/images/deep-q-learning/atari.png" alt="Atari"/></p>
<p>In <em>Q-Learning Algorithm</em>, there is a function called <strong>Q Function</strong>, which is used to approximate the reward based on a state. We call it <strong>Q(s,a)</strong>, where Q is a function which calculates the expected future value from state <em>s</em> and action <em>a</em>. Similarly in <em>Deep Q Network</em> algorithm, we use a neural network to approximate the reward based on the state. We will discuss how this works in detail.</p>
<h2 id="cartpole-game"><a href="#cartpole-game" class="anchor"></a>Cartpole Game</h2>
<p>Usually, training an agent to play an Atari game takes a while (from few hours to a day). So we will make an agent to play a simpler game called CartPole, but using the same idea used in the paper.</p>
<p>CartPole is one of the simplest environments in OpenAI gym (a game simulator). As you can see in the animation from the top, the goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right.</p>
<p>Gym makes interacting with the game environment really simple.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">next_state, reward, done, info </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> env.step(action)</span></span></code></pre></figure>
<p>As we discussed above, action can be either 0 or 1. If we pass those numbers, <code>env</code>, which represents the game environment, will emit the results. <code>done</code> is a boolean value telling whether the game ended or not. The old <code>state</code> information paired with <code>action</code> and <code>next_state</code> and <code>reward</code> is the information we need for training the agent.</p>
<h2 id="implementing-simple-neural-network-using-keras"><a href="#implementing-simple-neural-network-using-keras" class="anchor"></a>Implementing Simple Neural Network using Keras</h2>
<p>This post is not about deep learning or neural net. So we will consider neural net as just a black box algorithm that approximately maps inputs to outputs. It is basically an algorithm that learns on the pairs of examples input and output data, detects some kind of patterns, and predicts the output based on an unseen input data. Though neural network itself is not the focus of this article, we should understand how it is used in the DQN algorithm.</p>
<p><img src="/images/deep-q-learning/neuralnet.png" alt="Neural Network"/></p>
<p>Note that the neural net we are going to use is similar to the diagram above. We will have one input layer that receives 4 information and 3 hidden layers. But we are going to have 2 nodes in the output layer since there are two buttons (0 and 1) for the game.</p>
<p>Keras makes it really simple to implement a basic neural network. The code below creates an empty neural net model. <code>activation</code>, <code>loss</code> and <code>optimizer</code> are the parameters that define the characteristics of the neural network, but we are not going to discuss it here.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#6A737D"># Neural Net for Deep Q Learning</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Sequential() creates the foundation of the layers.</span></span>
<span data-line=""><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Sequential()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># &#x27;Dense&#x27; is the basic form of a neural network layer</span></span>
<span data-line=""><span style="color:#6A737D"># Input Layer of state size(4) and Hidden Layer with 24 nodes</span></span>
<span data-line=""><span style="color:#E1E4E8">model.add(Dense(</span><span style="color:#79B8FF">24</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input_dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.state_size, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;relu&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""><span style="color:#6A737D"># Hidden layer with 24 nodes</span></span>
<span data-line=""><span style="color:#E1E4E8">model.add(Dense(</span><span style="color:#79B8FF">24</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;relu&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""><span style="color:#6A737D"># Output Layer with # of actions: 2 nodes (left, right)</span></span>
<span data-line=""><span style="color:#E1E4E8">model.add(Dense(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.action_size, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;linear&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Create the model based on the information above</span></span>
<span data-line=""><span style="color:#E1E4E8">model.compile(</span><span style="color:#FFAB70">loss</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;mse&#x27;</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#FFAB70">              optimizer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">Adam(</span><span style="color:#FFAB70">lr</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.learning_rate))</span></span></code></pre></figure>
<p>In order for a neural net to understand and predict based on the environment data, we have to feed it the information. <code>fit()</code> method feeds input and output pairs to the model. Then the model will train on those data to approximate the output based on the input.</p>
<p>This training process makes the neural net to predict the reward value from a certain <code>state</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">model.fit(state, reward_value, </span><span style="color:#FFAB70">epochs</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">verbose</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span></code></pre></figure>
<p>After training, the model now can predict the output from unseen input. When you call <code>predict()</code> function on the model, the model will predict the reward of current state based on the data you trained. Like so:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">prediction </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> model.predict(state)</span></span></code></pre></figure>
<h2 id="implementing-mini-deep-q-network-dqn"><a href="#implementing-mini-deep-q-network-dqn" class="anchor"></a>Implementing Mini Deep Q Network (DQN)</h2>
<p>Normally in games, the <em>reward</em> directly relates to the score of the game. Imagine a situation where the pole from CartPole game is tilted to the right. The expected future reward of pushing right button will then be higher than that of pushing the left button since it could yield higher score of the game as the pole survives longer.</p>
<p>In order to logically represent this intuition and train it, we need to express this as a formula that we can optimize on. The loss is just a value that indicates how far our prediction is from the actual target. For example, the prediction of the model could indicate that it sees more value in pushing the right button when in fact it can gain more reward by pushing the left button. We want to decrease this gap between the prediction and the target (loss). We will define our loss function as follows:</p>
<div style="display:flex;justify-content:center;margin-bottom:1rem"><img src="/images/deep-q-learning/deep-q-learning.png" alt="Deep Q-Learning Formula" style="width:100%;max-width:500px"/></div>
<div style="text-align:center;font-style:italic;font-size:12px;margin-bottom:1rem"><p>Mathematical representation of Q-learning from Taehoon Kim&#x27;s <a href="https://www.slideshare.net/carpedm20/ai-67616630">slides</a></p></div>
<p>We first carry out an action <em>a</em>, and observe the reward <em>r</em> and resulting new state <em>s&#x27;</em>. Based on the result, we calculate the maximum target Q and then discount it so that the future reward is worth less than immediate reward (It is a same concept as interest rate for money. Immediate payment always worth more for same amount of money). Lastly, we add the current reward to the discounted future reward to get the target value. Subtracting our current prediction from the target gives the loss. Squaring this value allows us to punish the large loss value more and treat the negative values same as the positive values.</p>
<p>Keras takes care of the most of the difficult tasks for us. We just need to define our target. We can express the target in a magical one-liner in python.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">target </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reward </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> gamma </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> np.amax(model.predict(next_state))</span></span></code></pre></figure>
<p>Keras does all the work of subtracting the target from the neural network output and squaring it. It also applies the learning rate we defined while creating the neural network model. This all happens inside the <code>fit()</code> function. This function decreases the gap between our prediction to target by the learning rate. The approximation of the Q-value converges to the true Q-value as we repeat the updating process. The loss will decrease and score will grow higher.</p>
<p>The most notable features of the DQN algorithm are <em>memorize</em> and <em>replay</em> methods. Both are pretty simple concepts. The original DQN architecture contains a several more tweaks for better training, but we are going to stick to a simpler version for now.</p>
<h2 id="memorize"><a href="#memorize" class="anchor"></a>Memorize</h2>
<p>One of the challenges for DQN is that neural network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So we need a list of previous experiences and observations to re-train the model with the previous experiences. We will call this array of experiences <code>memory</code> and use <code>memorize()</code> function to append state, action, reward, and next state to the memory.</p>
<p>In our example, the memory list will have a form of:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [(state, action, reward, next_state, done)</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]</span></span></code></pre></figure>
<p>And memorize function will simply store states, actions and resulting rewards to the memory like below:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#F97583">def</span><span style="color:#B392F0"> memorize</span><span style="color:#E1E4E8">(self, state, action, reward, next_state, done):</span></span>
<span data-line=""><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.memory.append((state, action, reward, next_state, done))</span></span></code></pre></figure>
<p><code>done</code> is just a boolean that indicates if the state is the final state.</p>
<p>Simple right?</p>
<h2 id="replay"><a href="#replay" class="anchor"></a>Replay</h2>
<p>A method that trains the neural net with experiences in the <code>memory</code> is called <code>replay()</code>. First, we sample some experiences from the <code>memory</code> and call them <code>minibatch</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#E1E4E8">minibatch </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> random.sample(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.memory, batch_size)</span></span></code></pre></figure>
<p>The above code will make <code>minibatch</code>, which is just a randomly sampled elements of the memories of size <code>batch_size</code>. We set the batch size as 32 for this example.</p>
<p>To make the agent perform well in long-term, we need to take into account not only the immediate rewards but also the future rewards we are going to get. In order to do this, we are going to have a &#x27;discount rate&#x27; or &#x27;gamma&#x27;. This way the agent will learn to maximize the discounted future reward based on the given state.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#6A737D"># Sample minibatch from the memory</span></span>
<span data-line=""><span style="color:#E1E4E8">minibatch </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> random.sample(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.memory, batch_size)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># Extract informations from each memory</span></span>
<span data-line=""><span style="color:#F97583">for</span><span style="color:#E1E4E8"> state, action, reward, next_state, done </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> minibatch:</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # if done, make our target reward</span></span>
<span data-line=""><span style="color:#E1E4E8">    target </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reward</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">    if</span><span style="color:#F97583"> not</span><span style="color:#E1E4E8"> done:</span></span>
<span data-line=""><span style="color:#6A737D">      # predict the future discounted reward</span></span>
<span data-line=""><span style="color:#E1E4E8">      target </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reward </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.gamma </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> \</span></span>
<span data-line=""><span style="color:#E1E4E8">               np.amax(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.model.predict(next_state)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">])</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # make the agent to approximately map</span></span>
<span data-line=""><span style="color:#6A737D">    # the current state to future discounted reward</span></span>
<span data-line=""><span style="color:#6A737D">    # We&#x27;ll call that target_f</span></span>
<span data-line=""><span style="color:#E1E4E8">    target_f </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.model.predict(state)</span></span>
<span data-line=""><span style="color:#E1E4E8">    target_f[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][action] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> target</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # Train the Neural Net with the state and target_f</span></span>
<span data-line=""><span style="color:#79B8FF">    self</span><span style="color:#E1E4E8">.model.fit(state, target_f, </span><span style="color:#FFAB70">epochs</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">verbose</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span></code></pre></figure>
<h2 id="how-the-agent-decides-to-act"><a href="#how-the-agent-decides-to-act" class="anchor"></a>How The Agent Decides to Act</h2>
<p>Our agent will randomly select its action at first by a certain percentage, called &#x27;exploration rate&#x27; or &#x27;epsilon&#x27;. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. <code>np.argmax()</code> is the function that picks the highest value between two elements in the <code>act_values[0]</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#F97583">def</span><span style="color:#B392F0"> act</span><span style="color:#E1E4E8">(self, state):</span></span>
<span data-line=""><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> np.random.rand() </span><span style="color:#F97583">&lt;=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.epsilon:</span></span>
<span data-line=""><span style="color:#6A737D">        # The agent acts randomly</span></span>
<span data-line=""><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> env.action_space.sample()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # Predict the reward value based on the given state</span></span>
<span data-line=""><span style="color:#E1E4E8">    act_values </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.model.predict(state)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # Pick the action based on the predicted reward</span></span>
<span data-line=""><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> np.argmax(act_values[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">])</span></span></code></pre></figure>
<p><code>act_values[0]</code> looks like this: [0.67, 0.2], each numbers representing the reward of picking action 0 and 1. And argmax function picks the index with the highest value. In the example of [0.67, 0.2], argmax returns <strong>0</strong> because the value in the 0th index is the highest.</p>
<h2 id="hyper-parameters"><a href="#hyper-parameters" class="anchor"></a>Hyper Parameters</h2>
<p>There are some parameters that have to be passed to a reinforcement learning agent. You will see these over and over again.</p>
<ul>
<li><code>episodes</code> - a number of games we want the agent to play.</li>
<li><code>gamma</code> - aka decay or discount rate, to calculate the future discounted reward.</li>
<li><code>epsilon</code> - aka exploration rate, this is the rate in which an agent randomly decides its action rather than prediction.</li>
<li><code>epsilon_decay</code> - we want to decrease the number of explorations as it gets good at playing games.</li>
<li><code>epsilon_min</code> - we want the agent to explore at least this amount.</li>
<li><code>learning_rate</code> - Determines how much neural net learns in each iteration.</li>
</ul>
<h2 id="putting-it-all-together-coding-the-deep-q-learning-agent"><a href="#putting-it-all-together-coding-the-deep-q-learning-agent" class="anchor"></a>Putting It All Together: Coding The Deep Q-Learning Agent</h2>
<p>I explained each part of the agent in the above. The code below implements everything we&#x27;ve talked about as a nice and clean class called <code>DQNAgent</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#6A737D"># Deep Q-learning Agent</span></span>
<span data-line=""><span style="color:#F97583">class</span><span style="color:#B392F0"> DQNAgent</span><span style="color:#E1E4E8">:</span></span>
<span data-line=""><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, state_size, action_size):</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.state_size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> state_size</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.action_size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> action_size</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.memory </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> deque(</span><span style="color:#FFAB70">maxlen</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2000</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.gamma </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.95</span><span style="color:#6A737D">    # discount rate</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.epsilon </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1.0</span><span style="color:#6A737D">  # exploration rate</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.epsilon_min </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.01</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.epsilon_decay </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.995</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.learning_rate </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.001</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.model </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._build_model()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">    def</span><span style="color:#B392F0"> _build_model</span><span style="color:#E1E4E8">(self):</span></span>
<span data-line=""><span style="color:#6A737D">        # Neural Net for Deep-Q learning Model</span></span>
<span data-line=""><span style="color:#E1E4E8">        model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Sequential()</span></span>
<span data-line=""><span style="color:#E1E4E8">        model.add(Dense(</span><span style="color:#79B8FF">24</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input_dim</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.state_size, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;relu&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""><span style="color:#E1E4E8">        model.add(Dense(</span><span style="color:#79B8FF">24</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;relu&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""><span style="color:#E1E4E8">        model.add(Dense(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.action_size, </span><span style="color:#FFAB70">activation</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;linear&#x27;</span><span style="color:#E1E4E8">))</span></span>
<span data-line=""><span style="color:#E1E4E8">        model.compile(</span><span style="color:#FFAB70">loss</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">&#x27;mse&#x27;</span><span style="color:#E1E4E8">,</span></span>
<span data-line=""><span style="color:#FFAB70">                      optimizer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">Adam(</span><span style="color:#FFAB70">lr</span><span style="color:#F97583">=</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.learning_rate))</span></span>
<span data-line=""><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> model</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">    def</span><span style="color:#B392F0"> memorize</span><span style="color:#E1E4E8">(self, state, action, reward, next_state, done):</span></span>
<span data-line=""><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">.memory.append((state, action, reward, next_state, done))</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">    def</span><span style="color:#B392F0"> act</span><span style="color:#E1E4E8">(self, state):</span></span>
<span data-line=""><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> np.random.rand() </span><span style="color:#F97583">&lt;=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.epsilon:</span></span>
<span data-line=""><span style="color:#F97583">            return</span><span style="color:#E1E4E8"> random.randrange(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.action_size)</span></span>
<span data-line=""><span style="color:#E1E4E8">        act_values </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.model.predict(state)</span></span>
<span data-line=""><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> np.argmax(act_values[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># returns action</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">    def</span><span style="color:#B392F0"> replay</span><span style="color:#E1E4E8">(self, batch_size):</span></span>
<span data-line=""><span style="color:#E1E4E8">        minibatch </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> random.sample(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.memory, batch_size)</span></span>
<span data-line=""><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> state, action, reward, next_state, done </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> minibatch:</span></span>
<span data-line=""><span style="color:#E1E4E8">            target </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reward</span></span>
<span data-line=""><span style="color:#F97583">            if</span><span style="color:#F97583"> not</span><span style="color:#E1E4E8"> done:</span></span>
<span data-line=""><span style="color:#E1E4E8">              target </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> reward </span><span style="color:#F97583">+</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.gamma </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> \</span></span>
<span data-line=""><span style="color:#E1E4E8">                       np.amax(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.model.predict(next_state)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">])</span></span>
<span data-line=""><span style="color:#E1E4E8">            target_f </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.model.predict(state)</span></span>
<span data-line=""><span style="color:#E1E4E8">            target_f[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">][action] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> target</span></span>
<span data-line=""><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.model.fit(state, target_f, </span><span style="color:#FFAB70">epochs</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">verbose</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#F97583">        if</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.epsilon </span><span style="color:#F97583">&gt;</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.epsilon_min:</span></span>
<span data-line=""><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">.epsilon </span><span style="color:#F97583">*=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.epsilon_decay</span></span></code></pre></figure>
<h2 id="lets-train-the-agent"><a href="#lets-train-the-agent" class="anchor"></a>Let&#x27;s Train the Agent</h2>
<p>The training part is even shorter. I&#x27;ll explain in the comments.</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#F97583">if</span><span style="color:#79B8FF"> __name__</span><span style="color:#F97583"> ==</span><span style="color:#9ECBFF"> &quot;__main__&quot;</span><span style="color:#E1E4E8">:</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # initialize gym environment and the agent</span></span>
<span data-line=""><span style="color:#E1E4E8">    env </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> gym.make(</span><span style="color:#9ECBFF">&#x27;CartPole-v0&#x27;</span><span style="color:#E1E4E8">)</span></span>
<span data-line=""><span style="color:#E1E4E8">    agent </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DQNAgent(env)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    # Iterate the game</span></span>
<span data-line=""><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> e </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(episodes):</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">        # reset state in the beginning of each game</span></span>
<span data-line=""><span style="color:#E1E4E8">        state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> env.reset()</span></span>
<span data-line=""><span style="color:#E1E4E8">        state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.reshape(state, [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">])</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">        # time_t represents each frame of the game</span></span>
<span data-line=""><span style="color:#6A737D">        # Our goal is to keep the pole upright as long as possible until score of 500</span></span>
<span data-line=""><span style="color:#6A737D">        # the more time_t the more score</span></span>
<span data-line=""><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> time_t </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">500</span><span style="color:#E1E4E8">):</span></span>
<span data-line=""><span style="color:#6A737D">            # turn this on if you want to render</span></span>
<span data-line=""><span style="color:#6A737D">            # env.render()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">            # Decide action</span></span>
<span data-line=""><span style="color:#E1E4E8">            action </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> agent.act(state)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">            # Advance the game to the next frame based on the action.</span></span>
<span data-line=""><span style="color:#6A737D">            # Reward is 1 for every frame the pole survived</span></span>
<span data-line=""><span style="color:#E1E4E8">            next_state, reward, done, _ </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> env.step(action)</span></span>
<span data-line=""><span style="color:#E1E4E8">            next_state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.reshape(next_state, [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">])</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">            # memorize the previous state, action, reward, and done</span></span>
<span data-line=""><span style="color:#E1E4E8">            agent.memorize(state, action, reward, next_state, done)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">            # make next_state the new current state for the next frame.</span></span>
<span data-line=""><span style="color:#E1E4E8">            state </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> next_state</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">            # done becomes True when the game ends</span></span>
<span data-line=""><span style="color:#6A737D">            # ex) The agent drops the pole</span></span>
<span data-line=""><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> done:</span></span>
<span data-line=""><span style="color:#6A737D">                # print the score and break out of the loop</span></span>
<span data-line=""><span style="color:#79B8FF">                print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;episode: </span><span style="color:#79B8FF">{}</span><span style="color:#9ECBFF">/</span><span style="color:#79B8FF">{}</span><span style="color:#9ECBFF">, score: </span><span style="color:#79B8FF">{}</span><span style="color:#9ECBFF">&quot;</span></span>
<span data-line=""><span style="color:#E1E4E8">                      .format(e, episodes, time_t))</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#F97583">                break</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">        # train the agent with the experience of the episode</span></span>
<span data-line=""><span style="color:#E1E4E8">        agent.replay(</span><span style="color:#79B8FF">32</span><span style="color:#E1E4E8">)</span></span></code></pre></figure>
<h2 id="result"><a href="#result" class="anchor"></a>Result</h2>
<p>In the beginning, the agent explores by acting randomly.</p>
<p><img src="/images/deep-q-learning/100.png" alt="Beginning - 100 episodes"/></p>
<p>It goes through multiple phases of learning.</p>
<ol>
<li>The cart masters balancing the pole.</li>
<li>But goes out of bounds, ending the game.</li>
<li>It tries to move away from the bounds when it is too close to them, but drops the pole.</li>
<li>The cart masters balancing and controlling the pole.</li>
</ol>
<p>After several hundreds of episodes (took 10 min), it starts to learn how to maximize the score.</p>
<p><img src="/images/deep-q-learning/300.png" alt="After 300 episodes"/></p>
<p>The final result is the birth of a skillful CartPole game player!</p>
<p><img src="/images/deep-q-learning/animation.gif" alt="Final Animation"/></p>
<p>The code used for this article is on <a target="_blank" rel="noopener noreferrer" href="https://github.com/keon/deep-q-learning">GitHub</a>. I added the saved weights for those who want to skip the training part.</p>
<h2 id="references"><a href="#references" class="anchor"></a>References</h2>
<ul>
<li><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf">Human-level Control Through Deep Reinforcement Learning</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://www.slideshare.net/carpedm20/ai-67616630">딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기 DEVIEW 2016</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/rlcode/reinforcement-learning">Reinforcement Learning Examples by RLCode</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
</ul></article></section><!--$--><!--/$--><footer class="mb-4 mt-12"><div class="flex justify-between text-xs opacity-70"><div class="flex"><span class="relative group mr-3 last:mr-0"><a class="hover:opacity-100 transition-opacity" rel="noopener noreferrer" target="_blank" href="/rss" data-umami-event="click-rss">rss</a><span class="absolute bottom-full left-1/2 -translate-x-1/2 mb-2 px-2 py-1 text-[10px] border border-neutral-300 dark:border-neutral-600 bg-white dark:bg-black text-black dark:text-white opacity-0 group-hover:opacity-100 transition-opacity whitespace-nowrap pointer-events-none">Subscribe to RSS feed</span></span><span class="relative group mr-3 last:mr-0"><a class="hover:opacity-100 transition-opacity" rel="noopener noreferrer" target="_blank" href="https://github.com/keon" data-umami-event="click-github">github</a><span class="absolute bottom-full left-1/2 -translate-x-1/2 mb-2 px-2 py-1 text-[10px] border border-neutral-300 dark:border-neutral-600 bg-white dark:bg-black text-black dark:text-white opacity-0 group-hover:opacity-100 transition-opacity whitespace-nowrap pointer-events-none">View GitHub profile</span></span><span class="relative group mr-3 last:mr-0"><a class="hover:opacity-100 transition-opacity" rel="noopener noreferrer" target="_blank" href="https://www.linkedin.com/in/keon/" data-umami-event="click-linkedin">linkedin</a><span class="absolute bottom-full left-1/2 -translate-x-1/2 mb-2 px-2 py-1 text-[10px] border border-neutral-300 dark:border-neutral-600 bg-white dark:bg-black text-black dark:text-white opacity-0 group-hover:opacity-100 transition-opacity whitespace-nowrap pointer-events-none">Connect on LinkedIn</span></span><span class="relative group mr-3 last:mr-0"><a class="hover:opacity-100 transition-opacity" rel="noopener noreferrer" target="_blank" href="https://x.com/keonwkim" data-umami-event="click-x">x</a><span class="absolute bottom-full left-1/2 -translate-x-1/2 mb-2 px-2 py-1 text-[10px] border border-neutral-300 dark:border-neutral-600 bg-white dark:bg-black text-black dark:text-white opacity-0 group-hover:opacity-100 transition-opacity whitespace-nowrap pointer-events-none">Follow on X</span></span></div></div></footer><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></main><script src="/_next/static/chunks/62c7aeefb8086c23.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[78580,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"ThemeProvider\"]\n3:I[35762,[\"/_next/static/chunks/7f64245c6154235a.js\",\"/_next/static/chunks/1ca67093247280ae.js\"],\"\"]\n4:I[79202,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"ThemeToggle\"]\n5:I[22871,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"default\"]\n6:I[59801,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"default\"]\n7:I[28192,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"default\"]\n8:I[84399,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"Analytics\"]\n9:I[60685,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"SpeedInsights\"]\na:I[42100,[\"/_next/static/chunks/7f64245c6154235a.js\"],\"UmamiAnalytics\"]\nc:I[98427,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"OutletBoundary\"]\nd:\"$Sreact.suspense\"\nf:I[98427,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"ViewportBoundary\"]\n11:I[98427,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"MetadataBoundary\"]\n13:I[32257,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"default\"]\n:HL[\"/_next/static/chunks/abf9089aed6bac0a.css\",\"style\"]\n:HL[\"/_next/static/media/GeistMono_Variable.p.73882635.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"ytgr4ScHVKxxVOmg1QY7t\",\"c\":[\"\",\"writing\",\"deep-q-learning\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[[\"contentType\",\"writing\",\"d\"],{\"children\":[[\"slug\",\"deep-q-learning\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/abf9089aed6bac0a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/7f64245c6154235a.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"geistmono_157ca88a-module__TXdLRq__variable\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"font-mono antialiased max-w-2xl mx-4 mt-6 lg:mx-auto text-sm text-black bg-white dark:text-white dark:bg-black\",\"children\":[\"$\",\"$L2\",null,{\"attribute\":\"class\",\"defaultTheme\":\"system\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-4 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"-ml-[8px] mb-8 tracking-tight\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-20\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex flex-row items-center justify-between relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative\",\"id\":\"nav\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row space-x-0 pr-10\",\"children\":[[\"$\",\"$L3\",\"/\",{\"href\":\"/\",\"className\":\"transition-all hover:bg-black hover:text-white dark:hover:bg-white dark:hover:text-black flex align-middle relative py-1 px-2 m-1\",\"children\":\"home\"}],[\"$\",\"$L3\",\"/writing\",{\"href\":\"/writing\",\"className\":\"transition-all hover:bg-black hover:text-white dark:hover:bg-white dark:hover:text-black flex align-middle relative py-1 px-2 m-1\",\"children\":\"writing\"}]]}],[\"$\",\"$L4\",null,{}]]}]}]}],[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L7\",null,{}],[\"$\",\"$L8\",null,{}],[\"$\",\"$L9\",null,{}],[\"$\",\"$La\",null,{}]]}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$Lb\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/1ca67093247280ae.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$d\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@e\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L11\",null,{\"children\":[\"$\",\"$d\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L12\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"14:I[28388,[\"/_next/static/chunks/7f64245c6154235a.js\",\"/_next/static/chunks/1ca67093247280ae.js\"],\"RelativeTime\"]\nb:[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"title font-bold text-lg\",\"children\":\"Deep Q-Learning with Keras and Gym\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center mt-1 mb-6 text-xs\",\"children\":[\"$\",\"$L14\",null,{\"date\":\"2017-02-06\"}]}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L15\"}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"16:I[61212,[\"/_next/static/chunks/fec5ea6183d0e55b.js\",\"/_next/static/chunks/3b60e5197848a126.js\"],\"IconMark\"]\ne:null\n"])</script><script>self.__next_f.push([1,"12:[[\"$\",\"title\",\"0\",{\"children\":\"Keon Kim\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Keon's blog.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"manifest\",\"href\":\"/favicon/site.webmanifest\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Keon Kim\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Keon's blog.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:url\",\"content\":\"https://keon.github.io/\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Keon Kim\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Keon Kim\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Keon's blog.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon/favicon-16x16.png\",\"sizes\":\"16x16\",\"type\":\"image/png\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/favicon/favicon-32x32.png\",\"sizes\":\"32x32\",\"type\":\"image/png\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon/favicon.ico\",\"sizes\":\"any\"}],[\"$\",\"link\",\"17\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon/apple-touch-icon.png\",\"sizes\":\"180x180\",\"type\":\"image/png\"}],[\"$\",\"link\",\"18\",{\"rel\":\"android-chrome-192x192\",\"href\":\"/favicon/android-chrome-192x192.png\"}],[\"$\",\"link\",\"19\",{\"rel\":\"android-chrome-512x512\",\"href\":\"/favicon/android-chrome-512x512.png\"}],[\"$\",\"$L16\",\"20\",{}]]\n"])</script><script>self.__next_f.push([1,":HL[\"/images/deep-q-learning/animation.gif\",\"image\"]\n:HL[\"/images/deep-q-learning/rl.png\",\"image\"]\n:HL[\"/images/deep-q-learning/atari.png\",\"image\"]\n"])</script><script>self.__next_f.push([1,"15:[[\"$\",\"div\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"center\",\"marginBottom\":\"2rem\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/animation.gif\",\"alt\":\"CartPole Animation\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This blog post will demonstrate how deep reinforcement learning (deep Q-learning) can be implemented and applied to play a CartPole game using Keras and Gym, \",[\"$\",\"strong\",null,{\"children\":\"in less than 100 lines of code\"}],\"!\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I'll explain everything without requiring any prerequisite knowledge about reinforcement learning.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The code used for this article is on \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/keon/deep-q-learning\",\"children\":\"GitHub\"}],\".\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"reinforcement-learning\",\"children\":[[[\"$\",\"a\",\"link-reinforcement-learning\",{\"href\":\"#reinforcement-learning\",\"className\":\"anchor\"}]],\"Reinforcement Learning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/rl.png\",\"alt\":\"Reinforcement Learning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Reinforcement Learning\"}],\" is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Just like how we learn to ride a bicycle, this kind of AI learns by trial and error. As seen in the picture, the brain represents the AI agent, which acts on the environment. After each action, the agent receives the feedback. The feedback consists of the reward and next state of the environment. The reward is usually defined by a human. If we use the analogy of the bicycle, we can define reward as the distance from the original starting point.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"deep-reinforcement-learning\",\"children\":[[[\"$\",\"a\",\"link-deep-reinforcement-learning\",{\"href\":\"#deep-reinforcement-learning\",\"className\":\"anchor\"}]],\"Deep Reinforcement Learning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Google's DeepMind published its famous paper \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://arxiv.org/abs/1312.5602\",\"children\":[\"$\",\"em\",null,{\"children\":\"Playing Atari with Deep Reinforcement Learning\"}]}],\", in which they introduced a new algorithm called \",[\"$\",\"strong\",null,{\"children\":\"Deep Q Network\"}],\" (DQN for short) in 2013. It demonstrated how an AI agent can learn to play games by just observing the screen without any prior information about those games. The result turned out to be pretty impressive. This paper opened the era of what is called 'deep reinforcement learning', a mix of deep learning and reinforcement learning.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.youtube.com/watch?v=V1eYniJ0Rnk\",\"children\":\"Click to Watch: DeepMind's Atari Player\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/atari.png\",\"alt\":\"Atari\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In \",[\"$\",\"em\",null,{\"children\":\"Q-Learning Algorithm\"}],\", there is a function called \",[\"$\",\"strong\",null,{\"children\":\"Q Function\"}],\", which is used to approximate the reward based on a state. We call it \",[\"$\",\"strong\",null,{\"children\":\"Q(s,a)\"}],\", where Q is a function which calculates the expected future value from state \",[\"$\",\"em\",null,{\"children\":\"s\"}],\" and action \",[\"$\",\"em\",null,{\"children\":\"a\"}],\". Similarly in \",[\"$\",\"em\",null,{\"children\":\"Deep Q Network\"}],\" algorithm, we use a neural network to approximate the reward based on the state. We will discuss how this works in detail.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"cartpole-game\",\"children\":[[[\"$\",\"a\",\"link-cartpole-game\",{\"href\":\"#cartpole-game\",\"className\":\"anchor\"}]],\"Cartpole Game\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Usually, training an agent to play an Atari game takes a while (from few hours to a day). So we will make an agent to play a simpler game called CartPole, but using the same idea used in the paper.\"}],\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\"]\n"])</script><script>self.__next_f.push([1,":HL[\"/images/deep-q-learning/neuralnet.png\",\"image\"]\n:HL[\"/images/deep-q-learning/deep-q-learning.png\",\"image\"]\n:HL[\"/images/deep-q-learning/100.png\",\"image\"]\n:HL[\"/images/deep-q-learning/300.png\",\"image\"]\n17:[\"$\",\"p\",null,{\"children\":\"CartPole is one of the simplest environments in OpenAI gym (a game simulator). As you can see in the animation from the top, the goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right.\"}]\n18:[\"$\",\"p\",null,{\"children\":\"Gym makes interacting with the game environment really simple.\"}]\n19:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"next_state, reward, done, info \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" env.step(action)\"}]]}]}]}]}]\n1a:[\"$\",\"p\",null,{\"children\":[\"As we discussed above, action can be either 0 or 1. If we pass those numbers, \",[\"$\",\"code\",null,{\"children\":\"env\"}],\", which represents the game environment, will emit the results. \",[\"$\",\"code\",null,{\"children\":\"done\"}],\" is a boolean value telling whether the game ended or not. The old \",[\"$\",\"code\",null,{\"children\":\"state\"}],\" information paired with \",[\"$\",\"code\",null,{\"children\":\"action\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"next_state\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"reward\"}],\" is the information we need for training the agent.\"]}]\n1b:[\"$\",\"h2\",null,{\"id\":\"implementing-simple-neural-network-using-keras\",\"children\":[[[\"$\",\"a\",\"link-implementing-simple-neural-network-using-keras\",{\"href\":\"#implementing-simple-neural-network-using-keras\",\"className\":\"anchor\"}]],\"Implementing Simple Neural Network using Keras\"]}]\n1c:[\"$\",\"p\",null,{\"children\":\"This post is not about deep learning or neural net. So we will consider neural net as just a black box algorithm that approximately maps inputs to outputs. It is basically an algorithm that learns on the pairs of examples input and output data, detects some kind of patterns, and predicts the output based on an unseen input data. Though neural network itself is not the focus of this article, we should understand how it is used in the DQN algorithm.\"}]\n1d:[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/neuralnet.png\",\"alt\":\"Neural Network\"}]}]\n1e:[\"$\",\"p\",null,{\"children\":\"Note that the neural net we are going to use is similar to the diagram above. We will have one input layer that receives 4 information and 3 hidden layers. But we are going to have 2 nodes in the output layer since there are two buttons (0 and 1) for the game.\"}]\n1f:[\"$\",\"p\",null,{\"children\":[\"Keras makes it really simple to implement a basic neural network. The code below creates an empty neural net model. \",[\"$\",\"code\",null,{\"children\":\"activation\"}],\", \",[\"$\",\"code\",null,{\"children\":\"loss\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"optimizer\"}],\" are the parameters that define the characteristics of the neural network, but we are not going to discuss it here.\"]}]\n"])</script><script>self.__next_f.push([1,"20:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Neural Net for Deep Q Learning\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Sequential() creates the foundation of the layers.\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" Sequential()\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# 'Dense' is the basic form of a neural network layer\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Input Layer of state size(4) and Hidden Layer with 24 nodes\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model.add(Dense(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"24\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"input_dim\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".state_size, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'relu'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Hidden layer with 24 nodes\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model.add(Dense(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"24\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'relu'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Output Layer with # of actions: 2 nodes (left, right)\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model.add(Dense(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".action_size, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'linear'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Create the model based on the information above\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model.compile(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"loss\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'mse'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\",\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"              optimizer\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"Adam(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"lr\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".learning_rate))\"}]]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"21:[\"$\",\"p\",null,{\"children\":[\"In order for a neural net to understand and predict based on the environment data, we have to feed it the information. \",[\"$\",\"code\",null,{\"children\":\"fit()\"}],\" method feeds input and output pairs to the model. Then the model will train on those data to approximate the output based on the input.\"]}]\n22:[\"$\",\"p\",null,{\"children\":[\"This training process makes the neural net to predict the reward value from a certain \",[\"$\",\"code\",null,{\"children\":\"state\"}],\".\"]}]\n23:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"model.fit(state, reward_value, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"epochs\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"verbose\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]]}]}]}]}]\n24:[\"$\",\"p\",null,{\"children\":[\"After training, the model now can predict the output from unseen input. When you call \",[\"$\",\"code\",null,{\"children\":\"predict()\"}],\" function on the model, the model will predict the reward of current state based on the data you trained. Like so:\"]}]\n25:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"prediction \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" model.predict(state)\"}]]}]}]}]}]\n26:[\"$\",\"h2\",null,{\"id\":\"implementing-mini-deep-q-network-dqn\",\"children\":[[[\"$\",\"a\",\"link-implementing-mini-deep-q-network-dqn\",{\"href\":\"#implementing-mini-deep-q-network-dqn\",\"className\":\"anchor\"}]],\"Implementing Mini Deep Q Network (DQN)\"]}]\n27:[\"$\",\"p\",null,{\"children\":[\"Normally in games, the \",[\"$\",\"em\",null,{\"children\":\"reward\"}],\" directly relates to the score of the game. Imagine a situation where the pole from CartPole game is tilted to the right. The expected future reward of pushing right button will then be higher than that of pushing the left button since it could yield higher score of the game as the pole survives longer.\"]}]\n28:[\"$\",\"p\",null,{\"children\":\"In order to logically represent this intuition and train it, we need to express this as a formula that we can optimize on. The loss is just a value that indicates how far our prediction is from the actual target. For example, the prediction of the model could indicate that it sees more value in pushing the right button when in fact it can gain more reward by pushing the left button. We want to decrease this gap between the prediction and the target (loss). We will define our loss function as follows:\"}]\n29:[\"$\",\"div\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"center\",\"marginBottom\":\"1rem\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/deep-q-learning.png\",\"alt\":\"Deep Q-Learning Formula\",\"style\":{\"width\":\"100%\",\"maxWidth\":\"500px\"}}]}]\n2a:[\"$\",\"div\",null,{\"style\":{\"textAlign\":\"center\",\"fontStyle\":\"italic\",\"fontSize\":\"12px\",\"marginBottom\":\"1rem\"},\"children\":[\"$\",\"p\",null,{\"children\":[\"Mathematical representation of Q-learning from Taehoon Kim's \",[\"$\",\"a\",null,{\"href\":\"https://www.slideshare.net/carpedm20/ai-67"])</script><script>self.__next_f.push([1,"616630\",\"children\":\"slides\"}]]}]}]\n2b:[\"$\",\"p\",null,{\"children\":[\"We first carry out an action \",[\"$\",\"em\",null,{\"children\":\"a\"}],\", and observe the reward \",[\"$\",\"em\",null,{\"children\":\"r\"}],\" and resulting new state \",[\"$\",\"em\",null,{\"children\":\"s'\"}],\". Based on the result, we calculate the maximum target Q and then discount it so that the future reward is worth less than immediate reward (It is a same concept as interest rate for money. Immediate payment always worth more for same amount of money). Lastly, we add the current reward to the discounted future reward to get the target value. Subtracting our current prediction from the target gives the loss. Squaring this value allows us to punish the large loss value more and treat the negative values same as the positive values.\"]}]\n2c:[\"$\",\"p\",null,{\"children\":\"Keras takes care of the most of the difficult tasks for us. We just need to define our target. We can express the target in a magical one-liner in python.\"}]\n2d:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"target \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" reward \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"+\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" gamma \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"*\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.amax(model.predict(next_state))\"}]]}]}]}]}]\n2e:[\"$\",\"p\",null,{\"children\":[\"Keras does all the work of subtracting the target from the neural network output and squaring it. It also applies the learning rate we defined while creating the neural network model. This all happens inside the \",[\"$\",\"code\",null,{\"children\":\"fit()\"}],\" function. This function decreases the gap between our prediction to target by the learning rate. The approximation of the Q-value converges to the true Q-value as we repeat the updating process. The loss will decrease and score will grow higher.\"]}]\n2f:[\"$\",\"p\",null,{\"children\":[\"The most notable features of the DQN algorithm are \",[\"$\",\"em\",null,{\"children\":\"memorize\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"replay\"}],\" methods. Both are pretty simple concepts. The original DQN architecture contains a several more tweaks for better training, but we are going to stick to a simpler version for now.\"]}]\n30:[\"$\",\"h2\",null,{\"id\":\"memorize\",\"children\":[[[\"$\",\"a\",\"link-memorize\",{\"href\":\"#memorize\",\"className\":\"anchor\"}]],\"Memorize\"]}]\n31:[\"$\",\"p\",null,{\"children\":[\"One of the challenges for DQN is that neural network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So we need a list of previous experiences and observations to re-train the model with the previous experiences. We will call this array of experiences \",[\"$\",\"code\",null,{\"children\":\"memory\"}],\" and use \",[\"$\",\"code\",null,{\"children\":\"memorize()\"}],\" function to append state, action, reward, and next state to the memory.\"]}]\n32:[\"$\",\"p\",null,{\"children\":\"In our example, the memory list will have a form of:\"}]\n33:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"memory \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" [(state, action, reward, next_state, done)\"}]"])</script><script>self.__next_f.push([1,",[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"...\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"]\"}]]}]}]}]}]\n34:[\"$\",\"p\",null,{\"children\":\"And memorize function will simply store states, actions and resulting rewards to the memory like below:\"}]\n35:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" memorize\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, state, action, reward, next_state, done):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"    self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory.append((state, action, reward, next_state, done))\"}]]}]]}]}]}]\n36:[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"done\"}],\" is just a boolean that indicates if the state is the final state.\"]}]\n37:[\"$\",\"p\",null,{\"children\":\"Simple right?\"}]\n38:[\"$\",\"h2\",null,{\"id\":\"replay\",\"children\":[[[\"$\",\"a\",\"link-replay\",{\"href\":\"#replay\",\"className\":\"anchor\"}]],\"Replay\"]}]\n39:[\"$\",\"p\",null,{\"children\":[\"A method that trains the neural net with experiences in the \",[\"$\",\"code\",null,{\"children\":\"memory\"}],\" is called \",[\"$\",\"code\",null,{\"children\":\"replay()\"}],\". First, we sample some experiences from the \",[\"$\",\"code\",null,{\"children\":\"memory\"}],\" and call them \",[\"$\",\"code\",null,{\"children\":\"minibatch\"}],\".\"]}]\n3a:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"minibatch \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" random.sample(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory, batch_size)\"}]]}]}]}]}]\n3b:[\"$\",\"p\",null,{\"children\":[\"The above code will make \",[\"$\",\"code\",null,{\"children\":\"minibatch\"}],\", which is just a randomly sampled elements of the memories of size \",[\"$\",\"code\",null,{\"children\":\"batch_size\"}],\". We set the batch size as 32 for this example.\"]}]\n3c:[\"$\",\"p\",null,{\"children\":\"To make the agent perform well in long-term, we need to take into account not only the immediate rewards but also the future rewards we are going to get. In order to do this, we are going to have a 'discount rate' or 'gamma'. This way the agent will learn to maximize the discounted future reward based on the given state.\"}]\n"])</script><script>self.__next_f.push([1,"3d:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Sample minibatch from the memory\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"minibatch \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" random.sample(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory, batch_size)\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Extract informations from each memory\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"for\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" state, action, reward, next_state, done \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"in\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" minibatch:\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # if done, make our target reward\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    target \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" reward\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\" not\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" done:\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"      # predict the future discounted reward\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"      target \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" reward \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"+\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".gamma \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"*\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" \\\\\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"               np.amax(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(next_state)[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # make the agent to approximately map\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # the current state to future discounted reward\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # We'll call that target_f\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    target_f \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(state)\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    target_f[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"][action] \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" target\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # Train the Neural Net with the state and target_f\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"    self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.fit(state, target_f, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"epochs\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],\"$L57\",\"$L58\",\"$L59\",\"$L5a\",\"$L5b\",\"$L5c\"]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"h2\",null,{\"id\":\"how-the-agent-decides-to-act\",\"children\":[[[\"$\",\"a\",\"link-how-the-agent-decides-to-act\",{\"href\":\"#how-the-agent-decides-to-act\",\"className\":\"anchor\"}]],\"How The Agent Decides to Act\"]}]\n3f:[\"$\",\"p\",null,{\"children\":[\"Our agent will randomly select its action at first by a certain percentage, called 'exploration rate' or 'epsilon'. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. \",[\"$\",\"code\",null,{\"children\":\"np.argmax()\"}],\" is the function that picks the highest value between two elements in the \",[\"$\",\"code\",null,{\"children\":\"act_values[0]\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" act\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, state):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.random.rand() \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"\u003c=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon:\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # The agent acts randomly\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        return\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" env.action_space.sample()\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # Predict the reward value based on the given state\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    act_values \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(state)\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # Pick the action based on the predicted reward\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    return\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.argmax(act_values[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])\"}]]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"act_values[0]\"}],\" looks like this: [0.67, 0.2], each numbers representing the reward of picking action 0 and 1. And argmax function picks the index with the highest value. In the example of [0.67, 0.2], argmax returns \",[\"$\",\"strong\",null,{\"children\":\"0\"}],\" because the value in the 0th index is the highest.\"]}]\n42:[\"$\",\"h2\",null,{\"id\":\"hyper-parameters\",\"children\":[[[\"$\",\"a\",\"link-hyper-parameters\",{\"href\":\"#hyper-parameters\",\"className\":\"anchor\"}]],\"Hyper Parameters\"]}]\n43:[\"$\",\"p\",null,{\"children\":\"There are some parameters that have to be passed to a reinforcement learning agent. You will see these over and over again.\"}]\n44:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"episodes\"}],\" - a number of games we want the agent to play.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"gamma\"}],\" - aka decay or discount rate, to calculate the future discounted reward.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"epsilon\"}],\" - aka exploration rate, this is the rate in which an agent randomly decides its action rather than prediction.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"epsilon_decay\"}],\" - we want to decrease the number of explorations as it gets good at playing games.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"epsilon_min\"}],\" - we want the agent to explore at least this amount.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"learning_rate\"}],\" - Determines how much neural net learns in each iteration.\"]}],\"\\n\"]}]\n45:[\"$\",\"h2\",null,{\"id\":\"putting-it-all-together-coding-the-deep-q-learning-agent\",\"children\":[[[\"$\",\"a\",\"link-putting-it-all-together-coding-the-deep-q-learning-agent\",{\"href\":\"#putting-it-all-together-coding-the-deep-q-learning-agent\",\"className\":\"anchor\"}]],\"Putting It All Together: Coding The Deep Q-Learning Agent\"]}]\n46:[\"$\",\"p\",null,{\"children\":[\"I explained each part of the agent in the above. The code below implements everything we've talked about as a nice and clean class called \",[\"$\",\"code\",null,{\"children\":\"DQNAgent\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# Deep Q-learning Agent\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"class\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" DQNAgent\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\":\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" __init__\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, state_size, action_size):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".state_size \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" state_size\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".action_size \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" action_size\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" deque(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"maxlen\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"2000\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".gamma \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" 0.95\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # discount rate\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" 1.0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"  # exploration rate\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon_min \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" 0.01\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon_decay \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" 0.995\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".learning_rate \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" 0.001\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"._build_model()\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" _build_model\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # Neural Net for Deep-Q learning Model\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        model \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" Sequential()\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        model.add(Dense(\"}],\"$L5d\",\"$L5e\",\"$L5f\",\"$L60\",\"$L61\",\"$L62\",\"$L63\",\"$L64\",\"$L65\",\"$L66\"]}],\"\\n\",\"$L67\",\"\\n\",\"$L68\",\"\\n\",\"$L69\",\"\\n\",\"$L6a\",\"\\n\",\"$L6b\",\"\\n\",\"$L6c\",\"\\n\",\"$L6d\",\"\\n\",\"$L6e\",\"\\n\",\"$L6f\",\"\\n\",\"$L70\",\"\\n\",\"$L71\",\"\\n\",\"$L72\",\"\\n\",\"$L73\",\"\\n\",\"$L74\",\"\\n\",\"$L75\",\"\\n\",\"$L76\",\"\\n\",\"$L77\",\"\\n\",\"$L78\",\"\\n\",\"$L79\",\"\\n\",\"$L7a\",\"\\n\",\"$L7b\",\"\\n\",\"$L7c\",\"\\n\",\"$L7d\",\"\\n\",\"$L7e\",\"\\n\",\"$L7f\",\"\\n\",\"$L80\",\"\\n\",\"$L81\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"h2\",null,{\"id\":\"lets-train-the-agent\",\"children\":[[[\"$\",\"a\",\"link-lets-train-the-agent\",{\"href\":\"#lets-train-the-agent\",\"className\":\"anchor\"}]],\"Let's Train the Agent\"]}]\n49:[\"$\",\"p\",null,{\"children\":\"The training part is even shorter. I'll explain in the comments.\"}]\n"])</script><script>self.__next_f.push([1,"4a:[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"python\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" __name__\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\" ==\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" \\\"__main__\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\":\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # initialize gym environment and the agent\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    env \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" gym.make(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'CartPole-v0'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"    agent \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" DQNAgent(env)\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"    # Iterate the game\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    for\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" e \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"in\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" range\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(episodes):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # reset state in the beginning of each game\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        state \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" env.reset()\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        state \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.reshape(state, [\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"4\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # time_t represents each frame of the game\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # Our goal is to keep the pole upright as long as possible until score of 500\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # the more time_t the more score\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        for\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" time_t \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"in\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" range\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"500\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"):\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # turn this on if you want to render\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # env.render()\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # Decide action\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            action \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" agent.act(state)\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # Advance the game to the next frame based on the action.\"}]}],\"\\n\",\"$L82\",\"\\n\",\"$L83\",\"\\n\",\"$L84\",\"\\n\",\"$L85\",\"\\n\",\"$L86\",\"\\n\",\"$L87\",\"\\n\",\"$L88\",\"\\n\",\"$L89\",\"\\n\",\"$L8a\",\"\\n\",\"$L8b\",\"\\n\",\"$L8c\",\"\\n\",\"$L8d\",\"\\n\",\"$L8e\",\"\\n\",\"$L8f\",\"\\n\",\"$L90\",\"\\n\",\"$L91\",\"\\n\",\"$L92\",\"\\n\",\"$L93\",\"\\n\",\"$L94\",\"\\n\",\"$L95\",\"\\n\",\"$L96\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"4b:[\"$\",\"h2\",null,{\"id\":\"result\",\"children\":[[[\"$\",\"a\",\"link-result\",{\"href\":\"#result\",\"className\":\"anchor\"}]],\"Result\"]}]\n4c:[\"$\",\"p\",null,{\"children\":\"In the beginning, the agent explores by acting randomly.\"}]\n4d:[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/100.png\",\"alt\":\"Beginning - 100 episodes\"}]}]\n4e:[\"$\",\"p\",null,{\"children\":\"It goes through multiple phases of learning.\"}]\n4f:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"The cart masters balancing the pole.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"But goes out of bounds, ending the game.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"It tries to move away from the bounds when it is too close to them, but drops the pole.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The cart masters balancing and controlling the pole.\"}],\"\\n\"]}]\n50:[\"$\",\"p\",null,{\"children\":\"After several hundreds of episodes (took 10 min), it starts to learn how to maximize the score.\"}]\n51:[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/300.png\",\"alt\":\"After 300 episodes\"}]}]\n52:[\"$\",\"p\",null,{\"children\":\"The final result is the birth of a skillful CartPole game player!\"}]\n53:[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/images/deep-q-learning/animation.gif\",\"alt\":\"Final Animation\"}]}]\n54:[\"$\",\"p\",null,{\"children\":[\"The code used for this article is on \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/keon/deep-q-learning\",\"children\":\"GitHub\"}],\". I added the saved weights for those who want to skip the training part.\"]}]\n55:[\"$\",\"h2\",null,{\"id\":\"references\",\"children\":[[[\"$\",\"a\",\"link-references\",{\"href\":\"#references\",\"className\":\"anchor\"}]],\"References\"]}]\n56:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://arxiv.org/abs/1312.5602\",\"children\":\"Playing Atari with Deep Reinforcement Learning\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf\",\"children\":\"Human-level Control Through Deep Reinforcement Learning\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.slideshare.net/carpedm20/ai-67616630\",\"children\":\"딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기 DEVIEW 2016\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/rlcode/reinforcement-learning\",\"children\":\"Reinforcement Learning Examples by RLCode\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.nervanasys.com/demystifying-deep-reinforcement-learning/\",\"children\":\"Demystifying Deep Reinforcement Learning\"}]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"57:[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"1\"}]\n58:[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}]\n59:[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"verbose\"}]\n5a:[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}]\n5b:[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}]\n5c:[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]\n5d:[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"24\"}]\n5e:[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}]\n5f:[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"input_dim\"}]\n60:[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}]\n61:[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}]\n62:[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".state_size, \"}]\n63:[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}]\n64:[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}]\n65:[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'relu'\"}]\n66:[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]\n67:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        model.add(Dense(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"24\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'relu'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]]}]\n68:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        model.add(Dense(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".action_size, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"activation\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'linear'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"))\"}]]}]\n69:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        model.compile(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"loss\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"'mse'\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\",\"}]]}]\n6a:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"                      optimizer\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"Adam(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"lr\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".learning_rate))\"}]]}]\n6b:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        return\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" model\"}]]}]\n6c:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n6d:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" memorize\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, state, action, reward, next_state, done):\"}]]}]\n6e:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"        self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory.append((state, action, reward, next_state, done))\"}]]}]\n6f:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n70:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    def\"}],[\"$\",\"span\""])</script><script>self.__next_f.push([1,",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" act\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, state):\"}]]}]\n71:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.random.rand() \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"\u003c=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon:\"}]]}]\n72:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"            return\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" random.randrange(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".action_size)\"}]]}]\n73:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        act_values \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(state)\"}]]}]\n74:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        return\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.argmax(act_values[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])  \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"# returns action\"}]]}]\n75:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n76:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"    def\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\" replay\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(self, batch_size):\"}]]}]\n77:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        minibatch \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" random.sample(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".memory, batch_size)\"}]]}]\n78:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        for\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" state, action, reward, next_state, done \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"in\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" minibatch:\"}]]}]\n79:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            target \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" reward\"}]]}]\n7a:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"            if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\" not\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" done:\"}]]}]\n7b:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"              target \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" reward \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"+\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".gamma \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"*\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" \\\\\"}]]}]\n7c:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"                       np.amax(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"self\"}],[\"$\",\"span"])</script><script>self.__next_f.push([1,"\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(next_state)[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])\"}]]}]\n7d:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            target_f \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.predict(state)\"}]]}]\n7e:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            target_f[\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"][action] \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" target\"}]]}]\n7f:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"            self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".model.fit(state, target_f, \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"epochs\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#FFAB70\"},\"children\":\"verbose\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]]}]\n80:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"        if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"\u003e\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon_min:\"}]]}]\n81:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"            self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"*=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" self\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\".epsilon_decay\"}]]}]\n82:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # Reward is 1 for every frame the pole survived\"}]}]\n83:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            next_state, reward, done, _ \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" env.step(action)\"}]]}]\n84:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            next_state \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" np.reshape(next_state, [\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"4\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"])\"}]]}]\n85:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n86:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # memorize the previous state, action, reward, and done\"}]}]\n87:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            agent.memorize(state, action, reward, next_state, done)\"}]}]\n88:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n89:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,"])</script><script>self.__next_f.push([1,"{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # make next_state the new current state for the next frame.\"}]}]\n8a:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"            state \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" next_state\"}]]}]\n8b:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n8c:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # done becomes True when the game ends\"}]}]\n8d:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"            # ex) The agent drops the pole\"}]}]\n8e:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"            if\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" done:\"}]]}]\n8f:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"                # print the score and break out of the loop\"}]}]\n90:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"                print\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"\\\"episode: \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"{}\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"/\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"{}\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\", score: \"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"{}\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\"\\\"\"}]]}]\n91:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"                      .format(e, episodes, time_t))\"}]}]\n92:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n93:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"                break\"}]}]\n94:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}]\n95:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"style\":{\"color\":\"#6A737D\"},\"children\":\"        # train the agent with the experience of the episode\"}]}]\n96:[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"        agent.replay(\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\"32\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\")\"}]]}]\n"])</script></body></html>